{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashioniq_vector_file_names =  [\"dress_one-hot.pickle\", \"shirt_one-hot.pickle\", \"toptee_one-hot.pickle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashioniq_imgname_to_vector = {}\n",
    "for f_name in fashioniq_vector_file_names:\n",
    "    f_path = os.path.join(\"/home/deokhk/coursework/CIGAR/data/image_retrieval\", f_name)\n",
    "    with open(f_path, \"rb\") as f:\n",
    "        vectors = pickle.load(f)\n",
    "    fashioniq_imgname_to_vector = {**fashioniq_imgname_to_vector, **vectors}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def closest_node(node, nodes):\n",
    "    closest_index = distance.cdist([node], nodes).argmin()\n",
    "    return nodes[closest_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_vector_file_name = \"lower_one-hot.pickle\"\n",
    "vector_path = os.path.join(\"/home/deokhk/coursework/CIGAR/data/image_retrieval\", lower_vector_file_name)\n",
    "with open(vector_path, \"rb\") as f:\n",
    "    lower_vectors = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fashioniq_dir = \"/home/deokhk/coursework/fashion-iq/data/captions/\"\n",
    "file_list = [\"cap.dress.train.json\", \"cap.dress.val.json\", \"cap.shirt.train.json\", \"cap.shirt.val.json\", \"cap.toptee.train.json\", \"cap.toptee.val.json\"]\n",
    "captions = []\n",
    "for file_name in file_list:\n",
    "    path = os.path.join(fashioniq_dir, file_name)\n",
    "    with open(path ,'r') as f:\n",
    "        cap = json.load(f)\n",
    "    captions += cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deokhk/anaconda3/envs/fashioniq/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "  0%|          | 0/24016 [00:00<?, ?it/s]/home/deokhk/anaconda3/envs/fashioniq/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "/home/deokhk/anaconda3/envs/fashioniq/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  0%|          | 13/24016 [00:08<4:11:59,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lower_vector_to_name = {v.tostring(): k for k, v in lower_vectors.items()}\n",
    "\n",
    "lower_vector_list = []\n",
    "for k ,v in lower_vectors.items():\n",
    "    lower_vector_list.append(v)\n",
    "\n",
    "# Returned generated data has a format like..\n",
    "# [{\"target\":, \"candidate\":, \"captions:\" [..], \"similarity\": []}]\n",
    "lower_augmented_caption_datas = []\n",
    "for caption in tqdm(captions):\n",
    "    discard = False\n",
    "    target = caption['target']\n",
    "    candidate = caption['candidate']\n",
    "    fashioniq_captions = caption['captions']\n",
    "\n",
    "    try:\n",
    "        target_vector = fashioniq_imgname_to_vector[target]\n",
    "    except KeyError:\n",
    "        discard = True\n",
    "        \n",
    "    try:\n",
    "        candidate_vector = fashioniq_imgname_to_vector[candidate]\n",
    "    except KeyError:\n",
    "        discard = True\n",
    "\n",
    "    if not discard:\n",
    "        target_closest_vector = closest_node(target_vector, lower_vector_list)\n",
    "        candidate_closest_vector = closest_node(candidate_vector, lower_vector_list)\n",
    "        lower_target_imageid = lower_vector_to_name[target_closest_vector.tostring()]\n",
    "        candidate_target_imageid = lower_vector_to_name[candidate_closest_vector.tostring()]\n",
    "\n",
    "        vector_diff_fashioniq = target_vector - candidate_vector\n",
    "        vector_diff_lower = target_closest_vector - candidate_closest_vector\n",
    "\n",
    "        similarity = distance.cosine(vector_diff_fashioniq, vector_diff_lower)\n",
    "\n",
    "        generated_data_pair = dict()\n",
    "        generated_data_pair['target'] = lower_target_imageid\n",
    "        generated_data_pair['candidate'] = candidate_target_imageid\n",
    "        generated_data_pair['captions'] = fashioniq_captions\n",
    "        generated_data_pair['similarity'] = similarity\n",
    "        lower_augmented_caption_datas.append(generated_data_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'img/Cuffed_Origami-Front_Joggers/img_00000004.jpg',\n",
       "  'candidate': 'img/Lady_Lace_Mini_Skirt/img_00000027.jpg',\n",
       "  'captions': ['is longer and black wit a fitted waist',\n",
       "   'is black with straps'],\n",
       "  'similarity': 0.5256583509747431},\n",
       " {'target': 'img/Rose_Print_Leggings/img_00000010.jpg',\n",
       "  'candidate': 'img/Space_Dye_Capri_Leggings/img_00000065.jpg',\n",
       "  'captions': ['has long sleeves and stripes', 'is striped with sleeves'],\n",
       "  'similarity': 0.6150998205402495},\n",
       " {'target': 'img/Striped_Linen_Belted_Shorts/img_00000014.jpg',\n",
       "  'candidate': 'img/Dark_Wash_Jeggings/img_00000026.jpg',\n",
       "  'captions': ['is longer and yellow',\n",
       "   'belted maxi dress in block colors of yellow and green'],\n",
       "  'similarity': 0.18350341907227397},\n",
       " {'target': 'img/Coated_Clean_Wash_-_Skinny_Jeans/img_00000011.jpg',\n",
       "  'candidate': 'img/Striped_Tribal_Print_Sweatpants/img_00000001.jpg',\n",
       "  'captions': ['is a back patterned dress', 'is not as bright in color'],\n",
       "  'similarity': 0.3453463292920229},\n",
       " {'target': 'img/Embroidered_Crepe_Shorts/img_00000035.jpg',\n",
       "  'candidate': 'img/Street-Chic_Knit_Shorts/img_00000027.jpg',\n",
       "  'captions': ['has short sleeves and is longer and more flowing',\n",
       "   'has longer sleeves'],\n",
       "  'similarity': 0.4777670321329064}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_augmented_caption_datas[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def describe_threshold_distribution(augmented_lower_dataset):\n",
    "    thersholds = []\n",
    "    for elem in augmented_lower_dataset:\n",
    "        thersholds.append(elem['similarity'])\n",
    "    df = pd.DataFrame(thersholds)\n",
    "    df.describe()\n",
    "\n",
    "describe_threshold_distribution"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb418996a52327a98d4fafe11370b1855ec2fa9f6b1f641789f15dfa4a32dd98"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('fashioniq': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
